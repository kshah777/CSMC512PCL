{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_Script_PCL_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kshah777/CSMC512PCL/blob/main/Python_Script_PCL_Detection3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw03c_HPWUeK"
      },
      "source": [
        "Import Section : Import all required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et8a01wsWaXx",
        "outputId": "c90de23e-46ba-4386-e29c-08cc18d6914d"
      },
      "source": [
        "# Used to pull data from google drie\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import io\n",
        "from io import FileIO\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "# Used for reading csv file\n",
        "import pandas\n",
        "\n",
        "# Module for contraction expansion\n",
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "#Module will be used for punctuation Removal\n",
        "from string import punctuation\n",
        "\n",
        "#Tokenizer \n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import stopword list\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Import Lemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Used for random selection in subsampling\n",
        "import random\n",
        "\n",
        "#Import TF-IDF\n",
        "!pip install sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Import BERT Things\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertModel, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "\n",
        "# Import Neural network modules\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AdamW\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "print(\"Finished Importing\")\n"
      ],
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.58)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Finished Importing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98ud4dJ_XaLg"
      },
      "source": [
        "Set Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjPqnDR2Xcgf",
        "outputId": "15cf1e7a-325a-489a-e928-1e2fdcbe1554"
      },
      "source": [
        "#Data-PrePre-processing parameters\n",
        "remove_non_strings = True # Make this true please\n",
        "\n",
        "#Data-Pre-processing parameters\n",
        "expand_contractions = True\n",
        "remove_punctuation = True\n",
        "make_lowercase = True  #Bert and I think TFIDF is uncased, so regardless of this input to it will be lowercase.\n",
        "#tokenize = True      Not actually a choice\n",
        "remove_stop_words = True\n",
        "lemmatization = True\n",
        "#NOTE, if remove_stop_words or lemmatization = TRUE, tokenize must also equal TRUE\n",
        "\n",
        "#Post Pre-processing data splitting parameters:\n",
        "remove_too_long = True # This will remove values that are over 512 character and therefore would just be truncated later\n",
        "random_seed = 0       # If Random seed is set to zero, seed will be automically generated\n",
        "proportion_train_data = 0.8\n",
        "\n",
        "# Full Model Paramters\n",
        "TF_IDF = True\n",
        "BERT = True\n",
        "\n",
        "#TF-IDF Parameters\n",
        "tfidf_maxfeatures = 384   #Note, Bert will provide a vecotr of length 768\n",
        "                          #Note 2: 2 tfidf vectors made (pcl, nonpcl), so max features * 2 is true length of tfidf vector\n",
        "\n",
        "# BERT Parameters\n",
        "batch_size_param = 16\n",
        "max_len = 256  #Recommend keeping bellow 400, Allows for BERT Token characters to reach max wtih special characters 512\n",
        "truncate = True\n",
        "\n",
        "#Neural Network parameters\n",
        "num_epochs = 15\n",
        "learning_rate = 0.003\n",
        "batch_size_nn = 8\n",
        "\n",
        "\n",
        "#Select Gpu\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry9ec7SWWdoQ"
      },
      "source": [
        "Data Import from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr1nVOt5WhuG",
        "outputId": "4fadf5f4-a0fe-414e-a89c-fc91c31b145e"
      },
      "source": [
        "# Download File from Google Drive\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Import Data File\n",
        "file_id = '159fu3B8BaoKzXMbTHFDYXSHy1AuVA1f7'  # Training file on the Google Drive\n",
        "downloaded = io.FileIO(\"dontpatronizeme_pcl.tsv\", 'w')\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "  status, done = downloader.next_chunk()\n",
        "  print(\"Download {}%.\".format(int(status.progress() * 100)))"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download 100%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfL9HI8aX4_F"
      },
      "source": [
        "Read Data CSV, find indeces that contain non-string and remove. Make list of all strings that are above current max lenth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGmDEBZDYA_o",
        "outputId": "a19a1a1c-c438-4487-980e-c9b127bf84c0"
      },
      "source": [
        "#Reads data set, skipping header rows starting with the first line (fixed issue where rows were combining by adding other delimiter)\n",
        "input_df = pandas.read_csv(\"dontpatronizeme_pcl.tsv\", delimiter='\\t|\\\\t', header=None, names=['trash', 'trash2', 'trash3', 'text', 'label'], skiprows = 4)\n",
        "\n",
        "#Create lists containing the text and labels (indces will line up)\n",
        "input_instances = input_df.text.values\n",
        "input_labels = input_df.label.values\n",
        "\n",
        "#Convert to normal lists because I am more comfortable with them\n",
        "input_labels = input_labels.tolist()\n",
        "input_instances = input_instances.tolist()\n",
        "\n",
        "#Print initial number of instances\n",
        "print(\"Initial Number of Instances:\", len(input_instances))\n",
        "print(\"Same count labels vs text instances:\", len(input_labels)==len(input_instances), \"\\n\")\n",
        "\n",
        "# Find out which indeces are too long or are empty/dont contain strings (dataset has some noise)\n",
        "indeces_delete_Empty = []\n",
        "indeces_delete_tooLong = []\n",
        "for i in range(len(input_instances)):\n",
        "    if type(input_instances[i]) is not str:\n",
        "        indeces_delete_Empty.append(i)\n",
        "    # elif len(input_instances[i]) > max_len:\n",
        "    #     indeces_delete_tooLong.append(i)  \n",
        "print(\"Indeces that are empty/do not contain strings:\", len(indeces_delete_Empty))\n",
        "# print(\"Indeces that are currently too long (length > {}):\".format(max_len), len(indeces_delete_tooLong))\n",
        "\n",
        "# Remove non strings (acording to hyperparameter)\n",
        "if remove_non_strings:\n",
        "  for i in reversed(indeces_delete_Empty):\n",
        "    input_labels.pop(i)\n",
        "    input_instances.pop(i)\n",
        "    print(\"Deleted All Empty or non String entries\")\n",
        "\n",
        "#Print  number of instances after prepre-processing\n",
        "print(\"\\nPost-prepre processing Number of Instances:\", len(input_instances))\n",
        "print(\"Same count labels vs text instances:\", len(input_labels)==len(input_instances), \"\\n\")"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Number of Instances: 10636\n",
            "Same count labels vs text instances: True \n",
            "\n",
            "Indeces that are empty/do not contain strings: 1\n",
            "Deleted All Empty or non String entries\n",
            "\n",
            "Post-prepre processing Number of Instances: 10635\n",
            "Same count labels vs text instances: True \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2uTFBJZh5_S"
      },
      "source": [
        "Pre processing steps dependent on hyper paramters aboce"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8fHeybCh8tq",
        "outputId": "3bf5146c-da01-48f5-90ef-b2e593f8ad6c"
      },
      "source": [
        "# The following webstie was heavily referenced for the text preprocessing:\n",
        "# https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/\n",
        "\n",
        "# Also referenced geeksforgeeks.org for some of these processes\n",
        "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
        "# https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
        "\n",
        "# If hyper parameter, expand contraction\n",
        "\n",
        "unaltered_instances = np.copy(input_instances)\n",
        "\n",
        "if expand_contractions:\n",
        "  print(\"Removing Contractions\")\n",
        "  texts_altered = 0\n",
        "  text_fix = \" \"\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    text_fix = contractions.fix(text)  \n",
        "    if text_fix != text:\n",
        "      texts_altered += 1\n",
        "      input_instances[i] = text_fix\n",
        "  print(\"Contractions Contained and Expanded in {} of the text excerpts\\n\".format(texts_altered))\n",
        "\n",
        "# If hyper parameter, remove punctuation\n",
        "if remove_punctuation:\n",
        "  print(\"Removing Punctuation\")\n",
        "  for i in range(len(input_instances)):\n",
        "    text = \"\".join([i for i in input_instances[i] if i not in punctuation])\n",
        "    input_instances[i] = text\n",
        "  print(\"Finished Removing Punctuation\\n\")\n",
        "\n",
        "# If hyper parameter, make all lowercase\n",
        "if make_lowercase:\n",
        "  print(\"Making Everything Lowercase\")\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    input_instances[i] = text.lower()\n",
        "  print(\"Finished Making Things Lowercase\\n\")\n",
        "\n",
        "# Always tokenize\n",
        "tokenize = True\n",
        "if tokenize:\n",
        "  print(\"Tokenizing\")\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    text_token = nltk.word_tokenize(text)\n",
        "    input_instances[i] = text_token\n",
        "  print(\"Finished Tokenizing. Sentences Split to Arrays\\n\")\n",
        "\n",
        "\n",
        "# If hyper parameter, remove stop words (Maybe look at altering stop word list?)\n",
        "if remove_stop_words:\n",
        "  print(\"Removing Stop Words\")\n",
        "  print(\"The following Stop Words will be removed\")\n",
        "  stops = nltk.corpus.stopwords.words('english')\n",
        "  for word_ind in range(0,len(stops), 5):\n",
        "    if(len(stops) - word_ind > 5):\n",
        "      print(\"{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\".format(stops[word_ind],stops[word_ind+1],stops[word_ind+2],stops[word_ind+3],stops[word_ind+4]))\n",
        "    else:\n",
        "      print(stops[word_ind], sep= \"\\t\")\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    input_instances[i]= [j for j in text if j not in stops]\n",
        "  print(\"Stop Words Removed\\n\")\n",
        "      \n",
        "# If hyper parameter, Lemmatize\n",
        "if lemmatization:\n",
        "  print(\"Lemmatizing\")\n",
        "  lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
        "  count= 0\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    input_instances[i] = [lemmatizer.lemmatize(word) for word in text]\n",
        "  print(\"Lemmatizing Finished\\n\")\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing Contractions\n",
            "Contractions Contained and Expanded in 516 of the text excerpts\n",
            "\n",
            "Removing Punctuation\n",
            "Finished Removing Punctuation\n",
            "\n",
            "Making Everything Lowercase\n",
            "Finished Making Things Lowercase\n",
            "\n",
            "Tokenizing\n",
            "Finished Tokenizing. Sentences Split to Arrays\n",
            "\n",
            "Removing Stop Words\n",
            "The following Stop Words will be removed\n",
            "i       \tme      \tmy      \tmyself  \twe      \n",
            "our     \tours    \tourselves\tyou     \tyou're  \n",
            "you've  \tyou'll  \tyou'd   \tyour    \tyours   \n",
            "yourself\tyourselves\the      \thim     \this     \n",
            "himself \tshe     \tshe's   \ther     \thers    \n",
            "herself \tit      \tit's    \tits     \titself  \n",
            "they    \tthem    \ttheir   \ttheirs  \tthemselves\n",
            "what    \twhich   \twho     \twhom    \tthis    \n",
            "that    \tthat'll \tthese   \tthose   \tam      \n",
            "is      \tare     \twas     \twere    \tbe      \n",
            "been    \tbeing   \thave    \thas     \thad     \n",
            "having  \tdo      \tdoes    \tdid     \tdoing   \n",
            "a       \tan      \tthe     \tand     \tbut     \n",
            "if      \tor      \tbecause \tas      \tuntil   \n",
            "while   \tof      \tat      \tby      \tfor     \n",
            "with    \tabout   \tagainst \tbetween \tinto    \n",
            "through \tduring  \tbefore  \tafter   \tabove   \n",
            "below   \tto      \tfrom    \tup      \tdown    \n",
            "in      \tout     \ton      \toff     \tover    \n",
            "under   \tagain   \tfurther \tthen    \tonce    \n",
            "here    \tthere   \twhen    \twhere   \twhy     \n",
            "how     \tall     \tany     \tboth    \teach    \n",
            "few     \tmore    \tmost    \tother   \tsome    \n",
            "such    \tno      \tnor     \tnot     \tonly    \n",
            "own     \tsame    \tso      \tthan    \ttoo     \n",
            "very    \ts       \tt       \tcan     \twill    \n",
            "just    \tdon     \tdon't   \tshould  \tshould've\n",
            "now     \td       \tll      \tm       \to       \n",
            "re      \tve      \ty       \tain     \taren    \n",
            "aren't  \tcouldn  \tcouldn't\tdidn    \tdidn't  \n",
            "doesn   \tdoesn't \thadn    \thadn't  \thasn    \n",
            "hasn't  \thaven   \thaven't \tisn     \tisn't   \n",
            "ma      \tmightn  \tmightn't\tmustn   \tmustn't \n",
            "needn   \tneedn't \tshan    \tshan't  \tshouldn \n",
            "shouldn't\twasn    \twasn't  \tweren   \tweren't \n",
            "won\n",
            "Stop Words Removed\n",
            "\n",
            "Lemmatizing\n",
            "Lemmatizing Finished\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F2bNkG442E_"
      },
      "source": [
        "This Section will now seperate the data into arrays for PCL containing and non PCL containing Data. Non PCL randomly samples to create equal arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynoRmTem4759",
        "outputId": "f06e27e1-0a5a-43e4-f1dd-d2069847a0a2"
      },
      "source": [
        "#Make sure data lengths didnt change following pre-processing\n",
        "print(\"Text after Pre Proccessing\\n------------------\")\n",
        "print(\"Post-prepre processing Number of Instances:\", len(input_instances))\n",
        "print(\"Same count labels and text instances?\", len(input_labels)==len(input_instances), \"\\n\")\n",
        "\n",
        "#Check for empty strings (shouldn't be any, but who know)\n",
        "# and check for strings that are too long(just remove them, smaller number now)\n",
        "indeces_delete_Empty = []\n",
        "indeces_delete_tooLong = []\n",
        "\n",
        "max_obs_len =0\n",
        "for i in range(len(input_instances)):\n",
        "    max_obs_len = max(max_obs_len, len(input_instances[i]))\n",
        "    if len(input_instances[i]) == 0:\n",
        "        indeces_delete_Empty.append(i)\n",
        "    elif len(input_instances[i]) > max_len:\n",
        "        indeces_delete_tooLong.append(i)  \n",
        "print(\"Indeces that are empty/do not contain strings:\", len(indeces_delete_Empty))\n",
        "print(\"Indeces that are currently too long (length > {}):\".format(max_len), len(indeces_delete_tooLong))\n",
        "\n",
        "\n",
        "print(\"Max observed length is...{}\".format(max_obs_len))\n",
        "\n",
        "unaltered_instances = unaltered_instances.tolist()\n",
        "#Delete empty strings and long strings according to parameters\n",
        "if remove_non_strings:\n",
        "  for i in reversed(indeces_delete_Empty):\n",
        "    input_labels.pop(i)\n",
        "    input_instances.pop(i)\n",
        "    unaltered_instances.pop(i)\n",
        "    print(\"Deleted All Empty or non String entries\")\n",
        "if remove_too_long:\n",
        "  for i in reversed(indeces_delete_tooLong):\n",
        "    input_labels.pop(i)\n",
        "    input_instances.pop(i)\n",
        "    unaltered_instances.pop(i)\n",
        "  print(\"Deleted All Strings Over {} character\\n\".format(max_len))\n",
        "\n",
        "print(\"Text after Deletions\\n------------------\")\n",
        "print(\"Post-deletion Number of Instances:\", len(input_instances))\n",
        "print(\"Same count labels and text instances?\", len(input_labels)==len(input_instances), \"\\n\")\n",
        "\n",
        "# Create list of indeces with pcl (3-4), no pcl (0-1), and count of abiguoius ones(2)\n",
        "pcl_indeces = []\n",
        "nonpcl_indeces = []\n",
        "count_ambiguous= 0\n",
        "for i in range(len(input_labels)):\n",
        "    label = input_labels[i]\n",
        "    if label>=3:\n",
        "        pcl_indeces.append(i)\n",
        "    elif label<2:\n",
        "        nonpcl_indeces.append(i)\n",
        "    else:\n",
        "        count_ambiguous +=1\n",
        "\n",
        "# Print diagnostic info to make sure  things acting as expected \n",
        "print(\"PCL occurencess:\", len(pcl_indeces))\n",
        "print(\"Non-PCL occurences:\", len(nonpcl_indeces))\n",
        "print(\"Removed for abiguity:\", count_ambiguous)\n",
        "print(\"Pcl+Non-Pcl+ambiguous = total instances?\", (len(pcl_indeces)+len(nonpcl_indeces)+count_ambiguous == len(input_instances)), \"(Expected to be true)\")\n",
        "\n",
        "# Set random seed if provided\n",
        "if random_seed:\n",
        "  print(\"Random seed is {}\".format(random_seed))\n",
        "  random.seed(random_seed)\n",
        "\n",
        "#shuffle the pcl_indeces\n",
        "random.shuffle(pcl_indeces)\n",
        "\n",
        "\n",
        "#randomly sample equal proportion of non-pcl language\n",
        "nonpcl_indeces_shortened = []\n",
        "nonpcl_indeces_shortened = random.sample(nonpcl_indeces, len(pcl_indeces))\n",
        "print(\"\\nNon-PCL Occurneces After Random Sample:\", len(nonpcl_indeces_shortened))\n",
        "\n",
        "\n",
        "# Combine the Items (which are currently arrays) back into sentences, if they were tokenized\n",
        "if tokenize:\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    input_instances[i] = \" \".join(text)\n",
        "\n",
        "  print(\"Sentences are back to strings\")\n"
      ],
      "execution_count": 169,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text after Pre Proccessing\n",
            "------------------\n",
            "Post-prepre processing Number of Instances: 10635\n",
            "Same count labels and text instances? True \n",
            "\n",
            "Indeces that are empty/do not contain strings: 0\n",
            "Indeces that are currently too long (length > 256): 2\n",
            "Max observed length is...499\n",
            "Deleted All Strings Over 256 character\n",
            "\n",
            "Text after Deletions\n",
            "------------------\n",
            "Post-deletion Number of Instances: 10633\n",
            "Same count labels and text instances? True \n",
            "\n",
            "PCL occurencess: 852\n",
            "Non-PCL occurences: 9632\n",
            "Removed for abiguity: 149\n",
            "Pcl+Non-Pcl+ambiguous = total instances? True (Expected to be true)\n",
            "\n",
            "Non-PCL Occurneces After Random Sample: 852\n",
            "Sentences are back to strings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJr1Gv7KHuEM"
      },
      "source": [
        "Will now split the groups into test and train data, ratio specified by user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSIvxosQH5BX",
        "outputId": "ac931edb-f243-41ab-cddb-406401ec1dce"
      },
      "source": [
        "#calculate cutoff value which will seperate user defined value to test/train\n",
        "test_index_cutoff = int(len(pcl_indeces)*proportion_train_data)\n",
        "print(\"{:.1%} cutoff at index: {}\".format(proportion_train_data, test_index_cutoff))\n",
        "\n",
        "\n",
        "#Create train/test lists for instances and labels using the  index lists\n",
        "train_instances = []\n",
        "train_labels = []\n",
        "train_instances_unaltered = []\n",
        "test_instances = []\n",
        "test_labels = []\n",
        "test_instances_unaltered = []\n",
        "\n",
        "for i in range(len(pcl_indeces)):\n",
        "    pcl_index = pcl_indeces[i]\n",
        "    nonpcl_index = nonpcl_indeces_shortened[i]\n",
        "    if i < test_index_cutoff:\n",
        "        train_instances.append(input_instances[pcl_index])\n",
        "        train_instances_unaltered.append(unaltered_instances[pcl_index])\n",
        "        train_labels.append(input_labels[pcl_index])\n",
        "        train_instances.append(input_instances[nonpcl_index])\n",
        "        train_instances_unaltered.append(unaltered_instances[nonpcl_index])\n",
        "        train_labels.append(input_labels[nonpcl_index])\n",
        "    else:\n",
        "        test_instances.append(input_instances[pcl_index])\n",
        "        test_instances_unaltered.append(unaltered_instances[pcl_index])\n",
        "        test_labels.append(input_labels[pcl_index])\n",
        "        test_instances.append(input_instances[nonpcl_index])\n",
        "        test_instances_unaltered.append(unaltered_instances[nonpcl_index])\n",
        "        test_labels.append(input_labels[nonpcl_index])\n",
        "\n",
        "#Print Diagnostic info to make sure things are behaving as expected\n",
        "print(\"\\nTest Instances:\",len(test_instances))\n",
        "print(\"Unaltered Test Instances: {}\".format(len(test_instances_unaltered)))\n",
        "print(\"Test Label:\", len(test_labels))\n",
        "print(\"Train Instances:\",len(train_instances))\n",
        "print(\"Unaltered Train Instances: {}\".format(len(train_instances_unaltered)))\n",
        "print(\"Train Label:\", len(train_labels))\n",
        "\n",
        "print(\"\\nExpected Combined Length:\", len(pcl_indeces)+len(nonpcl_indeces_shortened))\n",
        "print(\"Actual Combined Length:\", len(train_instances)+ len(test_instances))\n",
        "\n",
        "print(\"\\nTransform labels to 1 and 0\\n1=pcl, 0 = no pcl \")\n",
        "#Replace 1->0, 3and 4 to 1. Only other possible input is 0 which should remain\n",
        "for i in range(len(train_labels)):\n",
        "    if train_labels[i] == 1:\n",
        "        train_labels[i] = 0\n",
        "    elif train_labels[i] == 3:\n",
        "        train_labels[i] = 1\n",
        "    elif train_labels[i] == 4:\n",
        "        train_labels[i] = 1\n",
        "for i in range(len(test_labels)):\n",
        "    if test_labels[i] == 1:\n",
        "        test_labels[i] = 0\n",
        "    elif test_labels[i] == 3:\n",
        "        test_labels[i] = 1\n",
        "    elif test_labels[i] == 4:\n",
        "        test_labels[i] = 1\n",
        "\n",
        "print(\"\\n\\nFinal Diagnostic Information After seperation:\")\n",
        "print(\"Are train label and instances the same length? {}\\t Length is {}\".format(len(train_instances)==len(train_labels), len(train_instances)))\n",
        "print(\"Are test label and instances the same length? {}\\t Length is {}\".format(len(test_instances)==len(test_labels), len(test_instances)))\n",
        "print(\"Proportion of PCL in train: {}\".format(train_labels.count(1)/len(train_labels)))\n",
        "print(\"Proportion of PCL in test: {}\".format(test_labels.count(1)/len(test_labels)))\n",
        "\n"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80.0% cutoff at index: 681\n",
            "\n",
            "Test Instances: 342\n",
            "Unaltered Test Instances: 342\n",
            "Test Label: 342\n",
            "Train Instances: 1362\n",
            "Unaltered Train Instances: 1362\n",
            "Train Label: 1362\n",
            "\n",
            "Expected Combined Length: 1704\n",
            "Actual Combined Length: 1704\n",
            "\n",
            "Transform labels to 1 and 0\n",
            "1=pcl, 0 = no pcl \n",
            "\n",
            "\n",
            "Final Diagnostic Information After seperation:\n",
            "Are train label and instances the same length? True\t Length is 1362\n",
            "Are test label and instances the same length? True\t Length is 342\n",
            "Proportion of PCL in train: 0.5\n",
            "Proportion of PCL in test: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY0rJXPdKcSa"
      },
      "source": [
        "TF - IDF Implementation          Adapted from: https://sanjayasubedi.com.np/machinelearning/nlp/text-classification-with-sklearn/ and https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.YXgb1L_MJH4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za_d2wb1Kkxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae69b169-47fa-4ed2-d93e-3a036de69066"
      },
      "source": [
        "# # Check if when training you are supposed to only include postive(PCL containing) textx\n",
        "\n",
        "if TF_IDF:\n",
        "  #Creates tfidf scores for the train values, will be called later\n",
        "\n",
        "  #Initialize Vectorizer for Tfidf which will choose 1000 most informative features\n",
        "  vectorizer_pcl = TfidfVectorizer(max_features = tfidf_maxfeatures, use_idf=True)\n",
        "  vectorizer_nopcl = TfidfVectorizer(max_features = tfidf_maxfeatures, use_idf=True)\n",
        "\n",
        "  tfidf_pcl_train = []\n",
        "  tfidf_nopcl_train = []\n",
        "  for i in range(len(train_labels)):\n",
        "    if train_labels[i] == 1:\n",
        "      tfidf_pcl_train.append(train_instances[i])\n",
        "    else:\n",
        "      tfidf_nopcl_train.append(train_instances[i])\n",
        "  train_vectorized_pcl = vectorizer_pcl.fit_transform(tfidf_pcl_train)\n",
        "  train_vectorized_nopcl = vectorizer_nopcl.fit_transform(tfidf_nopcl_train)\n",
        "\n",
        "  #This is example of creating normalarray which will need to be appended to BERT array before NN\n",
        "  #print(vectorizer.transform([test_instances[0]]).toarray())\n",
        "  print(\"TFIDF Matrix Created\")\n",
        "else:\n",
        "  print(\"TFIDF Not created due to earlier parameters\")\n",
        "\n",
        "\n",
        "if TF_IDF:   #Behind if statement because only want to run if TF-IDF being created\n",
        "  # Create Array of tfidf result for each train instances:\n",
        "  train_tfidf_pcl = vectorizer_pcl.transform(train_instances).toarray()\n",
        "  train_tfidf_nopcl = vectorizer_nopcl.transform(train_instances).toarray()\n",
        "\n",
        "  #COMBINE THOSE 2 together to be train_tfidf\n",
        "  train_tfidf = []\n",
        "  for i in range(len(train_tfidf_pcl)):\n",
        "    to_append = np.concatenate((train_tfidf_pcl[i], train_tfidf_nopcl[i]), axis=None)\n",
        "    train_tfidf.append(to_append)\n",
        "  print(\"TFIDF Results For Train Data Complete\")\n",
        "  print(\"Dimension is {} x {}\".format(len(train_tfidf), len(train_tfidf[0])))\n",
        "\n",
        "  # Make Test array right now too, cause why not\n",
        "  test_tfidf_pcl = vectorizer_pcl.transform(test_instances).toarray()\n",
        "  test_tfidf_nopcl = vectorizer_nopcl.transform(test_instances).toarray()\n",
        "\n",
        "  #COMBINE THOSE 2 together to be test_tfidf\n",
        "  test_tfidf = []\n",
        "  for i in range(len(test_tfidf_pcl)):\n",
        "    to_append = np.concatenate((test_tfidf_pcl[i], test_tfidf_nopcl[i]), axis=None)\n",
        "    test_tfidf.append(to_append)\n",
        "\n",
        "  print(\"TFIDF Results For Test Data Complete\")\n",
        "  print(\"Dimension is {} x {}\".format(len(test_tfidf), len(test_tfidf[0])))"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFIDF Matrix Created\n",
            "TFIDF Results For Train Data Complete\n",
            "Dimension is 1362 x 768\n",
            "TFIDF Results For Test Data Complete\n",
            "Dimension is 342 x 768\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUkrwe64b7E6"
      },
      "source": [
        "BERT For Classification Implementation: Adapted from https://mccormickml.com/2019/07/22/BERT-fine-tuning/#12-installing-the-hugging-face-library, also took notes from Amy Olex's Bert Implementation\n",
        "\n",
        "This Section Finds and sets the true max length used in the classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qsBvRmJcF0E"
      },
      "source": [
        "if BERT:\n",
        "  #Set train instances and test instances to the unmodified versions (no preproccessing has occured)\n",
        "  train_instances = train_instances_unaltered\n",
        "  test_instances = test_instances_unaltered\n",
        "\n",
        "  # Load the BERT tokenizer. Will be used to Re-tokenize data\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "\n",
        "  # ''' Shows how Bert Tokenized\n",
        "  # # Print the original sentence.\n",
        "  # print(' Original: ', train_instances[0])\n",
        "  # # Print the sentence split into tokens.\n",
        "  # print('Tokenized: ', tokenizer.tokenize(train_instances[0]))\n",
        "  # # Print the sentence mapped to token ids.\n",
        "  # print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_instances[0])))\n",
        "  # '''\n",
        "\n",
        "  max_obs_len = 0\n",
        "  # Check all train data and find highest token character\n",
        "  for text in train_instances:\n",
        "      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "      input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "      # Update the maximum sentence length.\n",
        "      max_obs_len = max(max_obs_len, len(input_ids))\n",
        "\n",
        "  for text in test_instances:\n",
        "    # Check all train data and find highest token character\n",
        "      input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "      # Update the maximum sentence length.\n",
        "      max_obs_len = max(max_obs_len, len(input_ids))\n",
        "\n",
        "\n",
        "  print('Max Observed sentence length: ', max_obs_len)\n",
        "\n",
        "  #Select the minimum of the maximum lengths. BERT will truncate any sentances longer than this. \n",
        "  max_len = min(max_obs_len, max_len)\n"
      ],
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McNS3P81svfa"
      },
      "source": [
        "Training Tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruRBr2W1sxLX"
      },
      "source": [
        "if BERT:\n",
        "  print(\"Tokenize all of the training sentences and map the tokens to thier word IDs.\")\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  train_input_ids = []\n",
        "  train_attention_masks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for text in train_instances:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length'\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          text,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = max_len,           # Pad & truncate all sentence\n",
        "                          pad_to_max_length=True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.    \n",
        "      train_input_ids.append(encoded_dict['input_ids'])\n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      train_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "\n",
        "  train_input_ids = torch.stack(train_input_ids).squeeze()\n",
        "  train_attention_masks = torch.stack(train_attention_masks).squeeze()\n",
        "  train_label_tensor = torch.tensor(train_labels)\n",
        "  \n",
        "  #Make train dataset\n",
        "  #train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "\n",
        "  # Check dimensionality (should be same)\n",
        "  # print(train_input_ids.shape)\n",
        "  # print(train_attention_masks.shape)\n",
        "  # print(train_label_tensor.shape)\n",
        "\n",
        "  train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_label_tensor)\n",
        "\n",
        "  # ''' Print to see what tensor looks like\n",
        "  # # Print sentence 0, now as a list of IDs.\n",
        "  # print('Original: ', train_instances[0])\n",
        "  # print('Token IDs:', train_input_ids[0])\n",
        "  # '''"
      ],
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cac8Ed9Gs-y2"
      },
      "source": [
        "Test Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8R2oxvatAdq"
      },
      "source": [
        "if BERT:\n",
        "  print(\"Tokenize all of the testing sentences and map the tokens to thier word IDs.\")\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  test_input_ids = []\n",
        "  test_attention_masks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for text in test_instances:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length'\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "      test_encoded_dict = tokenizer.encode_plus(\n",
        "                          text,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = max_len,           # Pad & truncate all sentences.\n",
        "                          truncation = True,\n",
        "                          pad_to_max_length=True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.    \n",
        "      test_input_ids.append(test_encoded_dict['input_ids'])\n",
        "      \n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      test_attention_masks.append(test_encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "\n",
        "  test_input_ids = torch.stack(test_input_ids).squeeze()\n",
        "  test_attention_masks = torch.stack(test_attention_masks).squeeze()\n",
        "  test_labels_tensor = torch.tensor(test_labels)\n",
        "\n",
        "  #Make test dataset\n",
        "  test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels_tensor)\n",
        "\n",
        "  # '''Test the Output\n",
        "  # # Print sentence 0, now as a list of IDs.\n",
        "  # print('Original: ', test_instances[0])\n",
        "  # print('Token IDs:', test_input_ids[0])\n",
        "  # '''"
      ],
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kab6IqzpueFu"
      },
      "source": [
        "Create Data Loader for train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZErKJI6DueXQ"
      },
      "source": [
        "if BERT:\n",
        "  #Create the DataLoaders for our training and validation sets.\n",
        "  #We'll take training samples in random order. \n",
        "  print(\"Making Train dataloader\")\n",
        "  train_dataloader = DataLoader(\n",
        "              train_dataset,  # The training samples.\n",
        "              sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "              batch_size = batch_size_param # Trains with this batch size.\n",
        "          )\n",
        "\n",
        "  print(\"Making Test dataloader\")\n",
        "  # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "  validation_dataloader = DataLoader(\n",
        "              test_dataset, # The validation samples.\n",
        "              sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
        "              batch_size = batch_size_param # Evaluate with this batch size.\n",
        "          )"
      ],
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgBfbAwtzl4K"
      },
      "source": [
        "Loading Bert and setting parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnzgGeZ0zoqk"
      },
      "source": [
        "if BERT:\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "\n",
        "  print(\"Loading BERT\")\n",
        "  model = BertModel.from_pretrained(\n",
        "      \"bert-base-uncased\" # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  )\n",
        "\n",
        "  # Tell pytorch to run this model on the GPU.\n",
        "  model.to(device)\n",
        "\n",
        "#Set seeds if random seed it set\n",
        "if random_seed:\n",
        "  np.random.seed(random_seed)\n",
        "  torch.manual_seed(random_seed)\n",
        "  torch.cuda.manual_seed_all(random_seed)"
      ],
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KwU3Ihs3SQM"
      },
      "source": [
        "Get BERT embeddings for train data\n",
        "BERT embedding is average vector for sentence. Result has length 768."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZgtURrH3T8j"
      },
      "source": [
        "if BERT:\n",
        "  model.eval()\n",
        "\n",
        "  #print(b_input_ids[0])\n",
        "\n",
        "  train_word_embed_eachinstance = []\n",
        "  count = 0\n",
        "  print(\"There will be {} batches\".format(len(train_dataloader)))\n",
        "  for batch in train_dataloader:\n",
        "    if count%5==0 and count!=0:\n",
        "      print(\"On batch number {}\".format(count))\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model_bert not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "        #train_cls_head.append(outputs[1])\n",
        "        #train_word_embed_eachinstance.append(outputs[0])\n",
        "        #print(len(outputs[0]))\n",
        "        for sentence in outputs[0]:\n",
        "          #print(len(sentence))\n",
        "          sentence_embed = []\n",
        "          for embed_index in range(len(sentence[0])):\n",
        "            index_total = 0\n",
        "            for word in sentence:\n",
        "              index_total += word[embed_index]\n",
        "            sentence_embed.append(index_total / len(word))\n",
        "          train_word_embed_eachinstance.append(sentence_embed)\n",
        "        count += 1\n",
        "\n",
        "  #Variable still called train_cls_head, but is average of every words embedding across sentence\n",
        "  train_cls_head = torch.FloatTensor(train_word_embed_eachinstance)\n",
        "  print(\"Number of Bert Embeddings created: {}\\t\\t Should equal number of training instances\".format(len(train_cls_head)))\n",
        "  print(\"Length of a single BERT embedding: {}\\t\\t Should be 768\".format(len(train_cls_head[0])))\n",
        "\n",
        "  print(\"Bert Vectors Created for all Train instances\")"
      ],
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qS3NAr1DSV8"
      },
      "source": [
        "Get BERT embeddings for the test instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7rXd4PpDVMU"
      },
      "source": [
        "if BERT:\n",
        "  test_word_embed_eachinstance = []\n",
        "  count = 0\n",
        "  print(\"There will be {} batches\".format(len(validation_dataloader)))\n",
        "  for batch in validation_dataloader:\n",
        "    if count%5==0 and count!=0:\n",
        "      print(\"On batch number {}\".format(count))\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    t_input_ids, t_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model_bert not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        outputs = model(t_input_ids, token_type_ids=None, attention_mask=t_input_mask)\n",
        "        \n",
        "        for sentence in outputs[0]:\n",
        "          #print(len(sentence))\n",
        "          sentence_embed = []\n",
        "          for embed_index in range(len(word)):\n",
        "            index_total = 0\n",
        "            for word in sentence:\n",
        "              index_total += word[embed_index]\n",
        "            sentence_embed.append(index_total / len(word))\n",
        "          test_word_embed_eachinstance.append(sentence_embed)\n",
        "        \n",
        "        count += 1\n",
        "\n",
        "\n",
        "  test_cls_head = torch.FloatTensor(test_word_embed_eachinstance)\n",
        "  print(\"Number of Bert Embeddings created: {}\\t\\t Should equal number of test instances\".format(len(test_cls_head)))\n",
        "  print(\"Length of a single BERT embedding: {}\\t\\t Should be 768\".format(len(test_cls_head[0])))\n",
        "  print(\"Bert Vectors Embeddings Created for all Test instances\")"
      ],
      "execution_count": 178,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u8a2yUFp0Rk"
      },
      "source": [
        "In This block, Will combine the vectors produced by tfidf with the tensors/vectors created by BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXqFrFgvp6D5",
        "outputId": "b1ee2447-54f3-4fe0-bcac-ebe9a85cb31b"
      },
      "source": [
        "train_combined_vectors = []\n",
        "test_combined_vectors = []\n",
        "\n",
        "#IF doing both BERT and Tf-IDF\n",
        "if TF_IDF and BERT: \n",
        "  #Turn BERT tensor to numpy array for easier work\n",
        "  #train_cls_head.numpy() \n",
        "  print(\"TFIDF and BERT Results Same Length: {}\".format(len(train_tfidf) == len(train_cls_head)))\n",
        "  #Check that these are same length, if they arent, something went wrong\n",
        "\n",
        "  if len(train_tfidf) != len(train_cls_head):\n",
        "    print(\"Train Vectors are not the same Length, errors have occured.\\n Quitting Process\")\n",
        "  if len(test_tfidf) != len(test_cls_head):\n",
        "    \n",
        "    print(\"Train Vectors are not the same Length, errors have occured.\\n Quitting Process\")\n",
        "\n",
        "  #Combine Train TFIDF Vector into 1 vector\n",
        "  for i in range(len(train_tfidf)):\n",
        "    tfidf_cur_index = train_tfidf[i]\n",
        "    bert_cur_index = train_cls_head[i]\n",
        "    # print(len(tfidf_cur_index))\n",
        "    # print(len(bert_cur_index))\n",
        "    combined_entry = np.concatenate((tfidf_cur_index, bert_cur_index))\n",
        "    train_combined_vectors.append(combined_entry)\n",
        "  print(\"Train TFIDF and Combined Vectors Created\")\n",
        "\n",
        "#Combine Train TFIDF Vector into 1 vector\n",
        "  for i in range(len(test_tfidf)):\n",
        "    tfidf_cur_index = test_tfidf[i]\n",
        "    bert_cur_index = test_cls_head[i]\n",
        "    # print(len(tfidf_cur_index))\n",
        "    # print(len(bert_cur_index))\n",
        "    combined_entry = np.concatenate((tfidf_cur_index, bert_cur_index))\n",
        "    test_combined_vectors.append(combined_entry)\n",
        "  print(\"Test TFIDF and Combined Vectors Created\")\n",
        "\n",
        "\n",
        "# If BERt is not selected:\n",
        "elif TF_IDF and not BERT:\n",
        "  train_cls_head = [[]]\n",
        "  test_cls_head = [[]]\n",
        "  for i in range(len(train_tfidf)):\n",
        "    tfidf_cur_index = train_tfidf[i]\n",
        "    train_combined_vectors.append(tfidf_cur_index)\n",
        "  for i in range(len(test_tfidf)):\n",
        "    tfidf_cur_index = test_tfidf[i]\n",
        "    test_combined_vectors.append(tfidf_cur_index)\n",
        "\n",
        "#IF ONLY Bert is selected\n",
        "elif BERT and not TF_IDF:\n",
        "  train_tfidf = [[]]\n",
        "  test_tfidf = [[]]\n",
        "  for i in range(len(train_cls_head)):\n",
        "    bert_cur_index = train_cls_head[i].numpy()\n",
        "    train_combined_vectors.append(bert_cur_index)\n",
        "  for i in range(len(test_cls_head)):\n",
        "    bert_cur_index = test_cls_head[i].numpy()\n",
        "    test_combined_vectors.append(bert_cur_index)\n",
        "\n",
        "print(\"Expected Length is Bert vector {} + TFIDF vector {} for a total length of {}\".format(len(train_cls_head[0]), len(train_tfidf[0]), (len(train_tfidf[0])+len(train_cls_head[0]))))\n",
        "print(\"Combined Train Vector is {} Long\".format(len(train_combined_vectors[0])))\n",
        "print(\"Combined Test Vector is {} Long\".format(len(test_combined_vectors[0])))"
      ],
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Expected Length is Bert vector 0 + TFIDF vector 768 for a total length of 768\n",
            "Combined Train Vector is 768 Long\n",
            "Combined Test Vector is 768 Long\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWaX-DMLxqH3"
      },
      "source": [
        "Neural Network Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpGDCRrqxue0",
        "outputId": "09b8b688-863e-4fac-f635-aa0a58b30906"
      },
      "source": [
        "#Create training loader for neural netowrk\n",
        "train_combined_vect_nn = torch.tensor(train_combined_vectors).squeeze()\n",
        "train_label_tensor_nn = torch.tensor(train_labels)\n",
        "\n",
        "\n",
        "train_data_nn = TensorDataset(train_combined_vect_nn, train_label_tensor_nn)\n",
        "train_sampler_nn = RandomSampler(train_data_nn)\n",
        "train_dataloader_nn = DataLoader(train_data_nn, sampler=train_sampler_nn, batch_size = batch_size_nn)\n",
        "print(\"Train Loader Created\")\n",
        "\n",
        "#BUILD NEURAL NETWORK WITH SOFTMAX CLASSICATION OUTPUT\n",
        "#NUMBER OF INPUTS = length of combined vectors\n",
        "num_inputs = len(train_combined_vectors[0]) \n",
        "\n",
        "# Build a feed-forward network\n",
        "# one_third_cut = int(num_inputs/3)\n",
        "# nn_model = nn.Sequential(nn.Linear(num_inputs, num_inputs),\n",
        "#                          nn.ReLU(),\n",
        "#                          nn.Linear(num_inputs, one_third_cut *2),\n",
        "#                          nn.ReLU(),\n",
        "#                          nn.Linear(one_third_cut *2, int(2* one_third_cut / 3 )),\n",
        "#                          nn.ReLU(),\n",
        "#                          nn.Linear(int(2* one_third_cut / 3 ), 1),\n",
        "#                          nn.Sigmoid())\n",
        "\n",
        "# nn_model = nn.Sequential(nn.Linear(num_inputs, int(num_inputs/2)),\n",
        "#                          nn.ReLU(),\n",
        "#                          nn.Linear(int(num_inputs/2), 1),\n",
        "#                          nn.Sigmoid())\n",
        "\n",
        "\n",
        "nn_model = nn.Sequential(nn.Linear(num_inputs, 1),\n",
        "                           nn.Sigmoid())\n",
        " \n",
        "print(nn_model)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.Adam(nn_model.parameters(), lr = learning_rate)\n",
        "\n",
        "# Train the Neural Network\n",
        "nn_model.train()\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    train_pred, train_act = [], []\n",
        "    print(\"Epoch {}\".format(e))\n",
        "    running_loss = 0\n",
        "    nn_model.train()\n",
        "    for step, batch in enumerate(train_dataloader_nn):\n",
        "        # if step%10==0:\n",
        "        #  print(\"Step: \" + str(step))\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        vectors, labels = batch\n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = nn_model(vectors.float()\n",
        "        )\n",
        "        for index in range(len(output)):\n",
        "          if output[index] > 0.5:\n",
        "            train_pred.append(1)\n",
        "            train_act.append(labels[index])\n",
        "          else:\n",
        "            train_pred.append(0)\n",
        "            train_act.append(labels[index])\n",
        "\n",
        "        labels = labels.unsqueeze(1).float()\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    else:\n",
        "        correct = 0\n",
        "        for i in range(len(train_pred)):\n",
        "          if train_pred[i] == train_act[i]:\n",
        "            correct +=1\n",
        "        print(f\"Average Training loss Over The Epoch: {running_loss/len(train_dataloader_nn)}\")\n",
        "        print(\"Epoch Accuracy was {}\".format(correct / len(train_pred)))\n",
        "  \n"
      ],
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loader Created\n",
            "Sequential(\n",
            "  (0): Linear(in_features=768, out_features=1, bias=True)\n",
            "  (1): Sigmoid()\n",
            ")\n",
            "Epoch 0\n",
            "Average Training loss Over The Epoch: 0.6701675155009442\n",
            "Epoch Accuracy was 0.6651982378854625\n",
            "Epoch 1\n",
            "Average Training loss Over The Epoch: 0.6164062882375996\n",
            "Epoch Accuracy was 0.7679882525697503\n",
            "Epoch 2\n",
            "Average Training loss Over The Epoch: 0.5774532392708182\n",
            "Epoch Accuracy was 0.7738619676945668\n",
            "Epoch 3\n",
            "Average Training loss Over The Epoch: 0.5460939247008653\n",
            "Epoch Accuracy was 0.7856093979441997\n",
            "Epoch 4\n",
            "Average Training loss Over The Epoch: 0.5207155604808651\n",
            "Epoch Accuracy was 0.7936857562408223\n",
            "Epoch 5\n",
            "Average Training loss Over The Epoch: 0.5009981040020435\n",
            "Epoch Accuracy was 0.8091042584434655\n",
            "Epoch 6\n",
            "Average Training loss Over The Epoch: 0.4835825344623878\n",
            "Epoch Accuracy was 0.8113069016152716\n",
            "Epoch 7\n",
            "Average Training loss Over The Epoch: 0.4673906817422276\n",
            "Epoch Accuracy was 0.8237885462555066\n",
            "Epoch 8\n",
            "Average Training loss Over The Epoch: 0.4543924892854969\n",
            "Epoch Accuracy was 0.8186490455212923\n",
            "Epoch 9\n",
            "Average Training loss Over The Epoch: 0.44506007241226775\n",
            "Epoch Accuracy was 0.8267254038179148\n",
            "Epoch 10\n",
            "Average Training loss Over The Epoch: 0.4339053699670479\n",
            "Epoch Accuracy was 0.828928046989721\n",
            "Epoch 11\n",
            "Average Training loss Over The Epoch: 0.42313117311711895\n",
            "Epoch Accuracy was 0.8303964757709251\n",
            "Epoch 12\n",
            "Average Training loss Over The Epoch: 0.416019160147996\n",
            "Epoch Accuracy was 0.8370044052863436\n",
            "Epoch 13\n",
            "Average Training loss Over The Epoch: 0.40730675932956717\n",
            "Epoch Accuracy was 0.8436123348017621\n",
            "Epoch 14\n",
            "Average Training loss Over The Epoch: 0.40104196163994527\n",
            "Epoch Accuracy was 0.8487518355359766\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cv4oydrDHHR"
      },
      "source": [
        "Test the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG4JvKW2DHW1",
        "outputId": "f722ffba-e101-4fda-d54c-0185efd20563"
      },
      "source": [
        "#Create Test loader for neural netowrk\n",
        "test_combined_vect_nn = torch.tensor(test_combined_vectors).squeeze()\n",
        "test_label_tensor_nn = torch.tensor(test_labels)\n",
        "\n",
        "# print(test_combined_vect_nn.size())\n",
        "# print(test_label_tensor_nn.size())\n",
        "\n",
        "test_data_nn = TensorDataset(test_combined_vect_nn, test_label_tensor_nn)\n",
        "test_sampler_nn = RandomSampler(test_data_nn)\n",
        "test_dataloader_nn = DataLoader(test_data_nn, sampler=test_sampler_nn, batch_size = 1)\n",
        "print(\"length dataloader {}\".format(len(test_dataloader_nn)))\n",
        "# Tracking variables for Neural Network Test (MODIFY TO CHANGE WHAT IS REPORTED)\n",
        "\n",
        "\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "count=0\n",
        "# Evaluate data for one epoch\n",
        "with torch.no_grad():\n",
        "  nn_model.eval()\n",
        "  for batch in test_dataloader_nn:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    t_vectors, t_labels = batch\n",
        "\n",
        "    output = nn_model(t_vectors.float())\n",
        "    \n",
        "    if output > 0.5:\n",
        "      predictions.append(1)\n",
        "      true_labels.append(t_labels.item())\n",
        "      #print(1)\n",
        "    else:\n",
        "      predictions.append(0)\n",
        "      true_labels.append(t_labels.item())\n",
        "      #print(0)\n",
        "\n",
        "    \n",
        "    count+=1\n",
        "\n",
        "print(\"Testing complete!\")\n",
        "true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] == true_labels[i] and predictions[i] == 1:\n",
        "    true_pos +=1\n",
        "  if predictions[i] == true_labels[i] and predictions[i] == 0:\n",
        "    true_neg +=1\n",
        "  if predictions[i] != true_labels[i] and predictions[i] == 1:\n",
        "    false_pos += 1\n",
        "  if predictions[i] != true_labels[i] and predictions[i] == 0:\n",
        "    false_neg +=1\n",
        "\n",
        "print(\"Accuracy Equals \\t{}\".format((true_pos+true_neg)/(true_pos+true_neg+false_pos+false_neg)))\n",
        "precision = true_pos/(true_pos+false_pos)\n",
        "print(\"Precision Equals \\t{}\".format(precision))\n",
        "recall = true_pos/(true_pos+false_neg)\n",
        "print(\"Recall Equals\\t{}\".format(recall))\n",
        "print(\"F-Score Equals \\t{}\".format(2*( (precision * recall) / (precision + recall) ) ) )\n",
        "\n",
        "print((true_pos+true_neg+false_pos+false_neg))\n",
        "print(len(predictions))\n"
      ],
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length dataloader 342\n",
            "Testing complete!\n",
            "Accuracy Equals \t0.6842105263157895\n",
            "Precision Equals \t0.6740331491712708\n",
            "Recall Equals\t0.7134502923976608\n",
            "F-Score Equals \t0.6931818181818181\n",
            "342\n",
            "342\n"
          ]
        }
      ]
    }
  ]
}