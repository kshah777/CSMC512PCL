{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_Script_PCL_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kshah777/CSMC512PCL/blob/main/Python_Script_PCL_Detection6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw03c_HPWUeK"
      },
      "source": [
        "Import Section : Import all required Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Et8a01wsWaXx",
        "outputId": "454a8eb5-df61-4242-f8bb-f4659ee834b3"
      },
      "source": [
        "# Used to pull data from google drie\n",
        "from google.colab import auth\n",
        "from googleapiclient.discovery import build\n",
        "import io\n",
        "from io import FileIO\n",
        "from googleapiclient.http import MediaIoBaseDownload\n",
        "\n",
        "# Used for reading csv file\n",
        "import pandas\n",
        "\n",
        "# Module for contraction expansion\n",
        "!pip install contractions\n",
        "import contractions\n",
        "\n",
        "#Module will be used for punctuation Removal\n",
        "from string import punctuation\n",
        "\n",
        "#Tokenizer \n",
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Import stopword list\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Import Lemmatizer\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Used for random selection in subsampling\n",
        "import random\n",
        "\n",
        "#Import TF-IDF\n",
        "!pip install sklearn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "#Import BERT Things\n",
        "!pip install transformers\n",
        "from transformers import BertTokenizer\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertModel, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import numpy as np\n",
        "\n",
        "# Import Neural network modules\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AdamW\n",
        "from torch import optim\n",
        "\n",
        "\n",
        "print(\"Finished Importing\")\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.0.58)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.0.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Finished Importing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98ud4dJ_XaLg"
      },
      "source": [
        "Set Hyper-Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjPqnDR2Xcgf",
        "outputId": "8cf4e5d7-bd46-47db-d051-93814e3a83fd"
      },
      "source": [
        "#Data-PrePre-processing parameters\n",
        "remove_non_strings = True # Make this true please\n",
        "\n",
        "#Data-Pre-processing parameters\n",
        "expand_contractions = True\n",
        "remove_punctuation = True\n",
        "make_lowercase = True  #Bert and I think TFIDF is uncased, so regardless of this input to it will be lowercase.\n",
        "#tokenize = True      Not actually a choice\n",
        "remove_stop_words = True\n",
        "lemmatization = True\n",
        "#NOTE, if remove_stop_words or lemmatization = TRUE, tokenize must also equal TRUE\n",
        "\n",
        "#Post Pre-processing data splitting parameters:\n",
        "remove_too_long = True # This will remove values that are over 512 character and therefore would just be truncated later\n",
        "random_seed = 0       # If Random seed is set to zero, seed will be automically generated\n",
        "proportion_train_data = 0.8\n",
        "\n",
        "# Full Model Paramters\n",
        "TF_IDF = True\n",
        "BERT = True\n",
        "\n",
        "#TF-IDF Parameters\n",
        "tfidf_maxfeatures = 384   #Note, Bert will provide a vecotr of length 768\n",
        "                          #Note 2: 2 tfidf vectors made (pcl, nonpcl), so max features * 2 is true length of tfidf vector\n",
        "\n",
        "# BERT Parameters\n",
        "pre_process_BERT = False \n",
        "batch_size_param = 16\n",
        "max_len = 256  #Recommend keeping bellow 400, Allows for BERT Token characters to reach max wtih special characters 512\n",
        "truncate = True\n",
        "\n",
        "#Neural Network parameters\n",
        "num_epochs = 15\n",
        "learning_rate = 0.003\n",
        "batch_size_nn = 8\n",
        "\n",
        "\n",
        "#Select Gpu\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "\n"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available, using the CPU instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry9ec7SWWdoQ"
      },
      "source": [
        "Data Import from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rr1nVOt5WhuG",
        "outputId": "ee0639ef-5510-44b7-819d-5ee3a1bf6dca"
      },
      "source": [
        "# Download File from Google Drive\n",
        "auth.authenticate_user()\n",
        "drive_service = build('drive', 'v3')\n",
        "\n",
        "# Import Data File\n",
        "file_id = '159fu3B8BaoKzXMbTHFDYXSHy1AuVA1f7'  # Training file on the Google Drive\n",
        "downloaded = io.FileIO(\"dontpatronizeme_pcl.tsv\", 'w')\n",
        "request = drive_service.files().get_media(fileId=file_id)\n",
        "downloader = MediaIoBaseDownload(downloaded, request)\n",
        "done = False\n",
        "while done is False:\n",
        "  status, done = downloader.next_chunk()\n",
        "  print(\"Download {}%.\".format(int(status.progress() * 100)))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download 100%.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfL9HI8aX4_F"
      },
      "source": [
        "Read Data CSV, find indeces that contain non-string and remove. Make list of all strings that are above current max lenth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGmDEBZDYA_o",
        "outputId": "f00e24c7-a98a-4336-a2cf-b248dc0227e6"
      },
      "source": [
        "#Reads data set, skipping header rows starting with the first line (fixed issue where rows were combining by adding other delimiter)\n",
        "input_df = pandas.read_csv(\"dontpatronizeme_pcl.tsv\", delimiter='\\t|\\\\t', header=None, names=['trash', 'trash2', 'trash3', 'text', 'label'], skiprows = 4)\n",
        "\n",
        "#Create lists containing the text and labels (indces will line up)\n",
        "input_instances = input_df.text.values\n",
        "input_labels = input_df.label.values\n",
        "\n",
        "#Convert to normal lists because I am more comfortable with them\n",
        "input_labels = input_labels.tolist()\n",
        "input_instances = input_instances.tolist()\n",
        "\n",
        "#Print initial number of instances\n",
        "print(\"Initial Number of Instances:\", len(input_instances))\n",
        "print(\"Same count labels vs text instances:\", len(input_labels)==len(input_instances), \"\\n\")\n",
        "\n",
        "# Find out which indeces are too long or are empty/dont contain strings (dataset has some noise)\n",
        "indeces_delete_Empty = []\n",
        "indeces_delete_tooLong = []\n",
        "for i in range(len(input_instances)):\n",
        "    if type(input_instances[i]) is not str:\n",
        "        indeces_delete_Empty.append(i)\n",
        "    # elif len(input_instances[i]) > max_len:\n",
        "    #     indeces_delete_tooLong.append(i)  \n",
        "print(\"Indeces that are empty/do not contain strings:\", len(indeces_delete_Empty))\n",
        "# print(\"Indeces that are currently too long (length > {}):\".format(max_len), len(indeces_delete_tooLong))\n",
        "\n",
        "# Remove non strings (acording to hyperparameter)\n",
        "if remove_non_strings:\n",
        "  for i in reversed(indeces_delete_Empty):\n",
        "    input_labels.pop(i)\n",
        "    input_instances.pop(i)\n",
        "    print(\"Deleted All Empty or non String entries\")\n",
        "\n",
        "#Print  number of instances after prepre-processing\n",
        "print(\"\\nPost-prepre processing Number of Instances:\", len(input_instances))\n",
        "print(\"Same count labels vs text instances:\", len(input_labels)==len(input_instances), \"\\n\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Number of Instances: 10636\n",
            "Same count labels vs text instances: True \n",
            "\n",
            "Indeces that are empty/do not contain strings: 1\n",
            "Deleted All Empty or non String entries\n",
            "\n",
            "Post-prepre processing Number of Instances: 10635\n",
            "Same count labels vs text instances: True \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2uTFBJZh5_S"
      },
      "source": [
        "Pre processing steps dependent on hyper paramters aboce"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8fHeybCh8tq",
        "outputId": "331bae8c-67ca-4986-d055-a5156639089b"
      },
      "source": [
        "# The following webstie was heavily referenced for the text preprocessing:\n",
        "# https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/\n",
        "\n",
        "# Also referenced geeksforgeeks.org for some of these processes\n",
        "# https://www.geeksforgeeks.org/removing-stop-words-nltk-python/\n",
        "# https://www.geeksforgeeks.org/python-lemmatization-with-nltk/\n",
        "\n",
        "# If hyper parameter, expand contraction\n",
        "\n",
        "unaltered_instances = np.copy(input_instances)\n",
        "\n",
        "if expand_contractions:\n",
        "  print(\"Removing Contractions\")\n",
        "  texts_altered = 0\n",
        "  text_fix = \" \"\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    text_fix = contractions.fix(text)  \n",
        "    if text_fix != text:\n",
        "      texts_altered += 1\n",
        "      input_instances[i] = text_fix\n",
        "  print(\"Contractions Contained and Expanded in {} of the text excerpts\\n\".format(texts_altered))\n",
        "\n",
        "# If hyper parameter, remove punctuation\n",
        "if remove_punctuation:\n",
        "  print(\"Removing Punctuation\")\n",
        "  for i in range(len(input_instances)):\n",
        "    text = \"\".join([i for i in input_instances[i] if i not in punctuation])\n",
        "    input_instances[i] = text\n",
        "  print(\"Finished Removing Punctuation\\n\")\n",
        "\n",
        "# If hyper parameter, make all lowercase\n",
        "if make_lowercase:\n",
        "  print(\"Making Everything Lowercase\")\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    input_instances[i] = text.lower()\n",
        "  print(\"Finished Making Things Lowercase\\n\")\n",
        "\n",
        "# Always tokenize\n",
        "tokenize = True\n",
        "if tokenize:\n",
        "  print(\"Tokenizing\")\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    text_token = nltk.word_tokenize(text)\n",
        "    input_instances[i] = text_token\n",
        "  print(\"Finished Tokenizing. Sentences Split to Arrays\\n\")\n",
        "\n",
        "\n",
        "# If hyper parameter, remove stop words (Maybe look at altering stop word list?)\n",
        "if remove_stop_words:\n",
        "  print(\"Removing Stop Words\")\n",
        "  print(\"The following Stop Words will be removed\")\n",
        "  stops = nltk.corpus.stopwords.words('english')\n",
        "  for word_ind in range(0,len(stops), 5):\n",
        "    if(len(stops) - word_ind > 5):\n",
        "      print(\"{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\".format(stops[word_ind],stops[word_ind+1],stops[word_ind+2],stops[word_ind+3],stops[word_ind+4]))\n",
        "    else:\n",
        "      print(stops[word_ind], sep= \"\\t\")\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    input_instances[i]= [j for j in text if j not in stops]\n",
        "  print(\"Stop Words Removed\\n\")\n",
        "      \n",
        "# If hyper parameter, Lemmatize\n",
        "if lemmatization:\n",
        "  print(\"Lemmatizing\")\n",
        "  lemmatizer = nltk.wordnet.WordNetLemmatizer()\n",
        "  count= 0\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    input_instances[i] = [lemmatizer.lemmatize(word) for word in text]\n",
        "  print(\"Lemmatizing Finished\\n\")\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing Contractions\n",
            "Contractions Contained and Expanded in 516 of the text excerpts\n",
            "\n",
            "Removing Punctuation\n",
            "Finished Removing Punctuation\n",
            "\n",
            "Making Everything Lowercase\n",
            "Finished Making Things Lowercase\n",
            "\n",
            "Tokenizing\n",
            "Finished Tokenizing. Sentences Split to Arrays\n",
            "\n",
            "Removing Stop Words\n",
            "The following Stop Words will be removed\n",
            "i       \tme      \tmy      \tmyself  \twe      \n",
            "our     \tours    \tourselves\tyou     \tyou're  \n",
            "you've  \tyou'll  \tyou'd   \tyour    \tyours   \n",
            "yourself\tyourselves\the      \thim     \this     \n",
            "himself \tshe     \tshe's   \ther     \thers    \n",
            "herself \tit      \tit's    \tits     \titself  \n",
            "they    \tthem    \ttheir   \ttheirs  \tthemselves\n",
            "what    \twhich   \twho     \twhom    \tthis    \n",
            "that    \tthat'll \tthese   \tthose   \tam      \n",
            "is      \tare     \twas     \twere    \tbe      \n",
            "been    \tbeing   \thave    \thas     \thad     \n",
            "having  \tdo      \tdoes    \tdid     \tdoing   \n",
            "a       \tan      \tthe     \tand     \tbut     \n",
            "if      \tor      \tbecause \tas      \tuntil   \n",
            "while   \tof      \tat      \tby      \tfor     \n",
            "with    \tabout   \tagainst \tbetween \tinto    \n",
            "through \tduring  \tbefore  \tafter   \tabove   \n",
            "below   \tto      \tfrom    \tup      \tdown    \n",
            "in      \tout     \ton      \toff     \tover    \n",
            "under   \tagain   \tfurther \tthen    \tonce    \n",
            "here    \tthere   \twhen    \twhere   \twhy     \n",
            "how     \tall     \tany     \tboth    \teach    \n",
            "few     \tmore    \tmost    \tother   \tsome    \n",
            "such    \tno      \tnor     \tnot     \tonly    \n",
            "own     \tsame    \tso      \tthan    \ttoo     \n",
            "very    \ts       \tt       \tcan     \twill    \n",
            "just    \tdon     \tdon't   \tshould  \tshould've\n",
            "now     \td       \tll      \tm       \to       \n",
            "re      \tve      \ty       \tain     \taren    \n",
            "aren't  \tcouldn  \tcouldn't\tdidn    \tdidn't  \n",
            "doesn   \tdoesn't \thadn    \thadn't  \thasn    \n",
            "hasn't  \thaven   \thaven't \tisn     \tisn't   \n",
            "ma      \tmightn  \tmightn't\tmustn   \tmustn't \n",
            "needn   \tneedn't \tshan    \tshan't  \tshouldn \n",
            "shouldn't\twasn    \twasn't  \tweren   \tweren't \n",
            "won\n",
            "Stop Words Removed\n",
            "\n",
            "Lemmatizing\n",
            "Lemmatizing Finished\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9F2bNkG442E_"
      },
      "source": [
        "This Section will now seperate the data into arrays for PCL containing and non PCL containing Data. Non PCL randomly samples to create equal arrays"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynoRmTem4759",
        "outputId": "845c42ab-6afa-4f14-ddc4-2389ad81b77d"
      },
      "source": [
        "#Make sure data lengths didnt change following pre-processing\n",
        "print(\"Text after Pre Proccessing\\n------------------\")\n",
        "print(\"Post-prepre processing Number of Instances:\", len(input_instances))\n",
        "print(\"Same count labels and text instances?\", len(input_labels)==len(input_instances), \"\\n\")\n",
        "\n",
        "#Check for empty strings (shouldn't be any, but who know)\n",
        "# and check for strings that are too long(just remove them, smaller number now)\n",
        "indeces_delete_Empty = []\n",
        "indeces_delete_tooLong = []\n",
        "\n",
        "max_obs_len =0\n",
        "for i in range(len(input_instances)):\n",
        "    max_obs_len = max(max_obs_len, len(input_instances[i]))\n",
        "    if len(input_instances[i]) == 0:\n",
        "        indeces_delete_Empty.append(i)\n",
        "    elif len(input_instances[i]) > max_len:\n",
        "        indeces_delete_tooLong.append(i)  \n",
        "print(\"Indeces that are empty/do not contain strings:\", len(indeces_delete_Empty))\n",
        "print(\"Indeces that are currently too long (length > {}):\".format(max_len), len(indeces_delete_tooLong))\n",
        "\n",
        "\n",
        "print(\"Max observed length is...{}\".format(max_obs_len))\n",
        "\n",
        "unaltered_instances = unaltered_instances.tolist()\n",
        "#Delete empty strings and long strings according to parameters\n",
        "if remove_non_strings:\n",
        "  for i in reversed(indeces_delete_Empty):\n",
        "    input_labels.pop(i)\n",
        "    input_instances.pop(i)\n",
        "    unaltered_instances.pop(i)\n",
        "    print(\"Deleted All Empty or non String entries\")\n",
        "if remove_too_long:\n",
        "  for i in reversed(indeces_delete_tooLong):\n",
        "    input_labels.pop(i)\n",
        "    input_instances.pop(i)\n",
        "    unaltered_instances.pop(i)\n",
        "  print(\"Deleted All Strings Over {} character\\n\".format(max_len))\n",
        "\n",
        "print(\"Text after Deletions\\n------------------\")\n",
        "print(\"Post-deletion Number of Instances:\", len(input_instances))\n",
        "print(\"Same count labels and text instances?\", len(input_labels)==len(input_instances), \"\\n\")\n",
        "\n",
        "# Create list of indeces with pcl (3-4), no pcl (0-1), and count of abiguoius ones(2)\n",
        "pcl_indeces = []\n",
        "nonpcl_indeces = []\n",
        "count_ambiguous= 0\n",
        "for i in range(len(input_labels)):\n",
        "    label = input_labels[i]\n",
        "    if label>=3:\n",
        "        pcl_indeces.append(i)\n",
        "    elif label<2:\n",
        "        nonpcl_indeces.append(i)\n",
        "    else:\n",
        "        count_ambiguous +=1\n",
        "\n",
        "# Print diagnostic info to make sure  things acting as expected \n",
        "print(\"PCL occurencess:\", len(pcl_indeces))\n",
        "print(\"Non-PCL occurences:\", len(nonpcl_indeces))\n",
        "print(\"Removed for abiguity:\", count_ambiguous)\n",
        "print(\"Pcl+Non-Pcl+ambiguous = total instances?\", (len(pcl_indeces)+len(nonpcl_indeces)+count_ambiguous == len(input_instances)), \"(Expected to be true)\")\n",
        "\n",
        "# Set random seed if provided\n",
        "if random_seed:\n",
        "  print(\"Random seed is {}\".format(random_seed))\n",
        "  random.seed(random_seed)\n",
        "\n",
        "#shuffle the pcl_indeces\n",
        "random.shuffle(pcl_indeces)\n",
        "\n",
        "\n",
        "#randomly sample equal proportion of non-pcl language\n",
        "nonpcl_indeces_shortened = []\n",
        "nonpcl_indeces_shortened = random.sample(nonpcl_indeces, len(pcl_indeces))\n",
        "print(\"\\nNon-PCL Occurneces After Random Sample:\", len(nonpcl_indeces_shortened))\n",
        "\n",
        "\n",
        "# Combine the Items (which are currently arrays) back into sentences, if they were tokenized\n",
        "if tokenize:\n",
        "  for i in range(len(input_instances)):\n",
        "    text = input_instances[i]\n",
        "    input_instances[i] = \" \".join(text)\n",
        "\n",
        "  print(\"Sentences are back to strings\")\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text after Pre Proccessing\n",
            "------------------\n",
            "Post-prepre processing Number of Instances: 10635\n",
            "Same count labels and text instances? True \n",
            "\n",
            "Indeces that are empty/do not contain strings: 0\n",
            "Indeces that are currently too long (length > 256): 2\n",
            "Max observed length is...499\n",
            "Deleted All Strings Over 256 character\n",
            "\n",
            "Text after Deletions\n",
            "------------------\n",
            "Post-deletion Number of Instances: 10633\n",
            "Same count labels and text instances? True \n",
            "\n",
            "PCL occurencess: 852\n",
            "Non-PCL occurences: 9632\n",
            "Removed for abiguity: 149\n",
            "Pcl+Non-Pcl+ambiguous = total instances? True (Expected to be true)\n",
            "\n",
            "Non-PCL Occurneces After Random Sample: 852\n",
            "Sentences are back to strings\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJr1Gv7KHuEM"
      },
      "source": [
        "Will now split the groups into test and train data, ratio specified by user"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSIvxosQH5BX",
        "outputId": "912180a8-8994-4d9b-a753-d93583bfdb7a"
      },
      "source": [
        "#calculate cutoff value which will seperate user defined value to test/train\n",
        "test_index_cutoff = int(len(pcl_indeces)*proportion_train_data)\n",
        "print(\"{:.1%} cutoff at index: {}\".format(proportion_train_data, test_index_cutoff))\n",
        "\n",
        "\n",
        "#Create train/test lists for instances and labels using the  index lists\n",
        "train_instances = []\n",
        "train_labels = []\n",
        "train_instances_unaltered = []\n",
        "test_instances = []\n",
        "test_labels = []\n",
        "test_instances_unaltered = []\n",
        "\n",
        "for i in range(len(pcl_indeces)):\n",
        "    pcl_index = pcl_indeces[i]\n",
        "    nonpcl_index = nonpcl_indeces_shortened[i]\n",
        "    if i < test_index_cutoff:\n",
        "        train_instances.append(input_instances[pcl_index])\n",
        "        train_instances_unaltered.append(unaltered_instances[pcl_index])\n",
        "        train_labels.append(input_labels[pcl_index])\n",
        "        train_instances.append(input_instances[nonpcl_index])\n",
        "        train_instances_unaltered.append(unaltered_instances[nonpcl_index])\n",
        "        train_labels.append(input_labels[nonpcl_index])\n",
        "    else:\n",
        "        test_instances.append(input_instances[pcl_index])\n",
        "        test_instances_unaltered.append(unaltered_instances[pcl_index])\n",
        "        test_labels.append(input_labels[pcl_index])\n",
        "        test_instances.append(input_instances[nonpcl_index])\n",
        "        test_instances_unaltered.append(unaltered_instances[nonpcl_index])\n",
        "        test_labels.append(input_labels[nonpcl_index])\n",
        "\n",
        "#Print Diagnostic info to make sure things are behaving as expected\n",
        "print(\"\\nTest Instances:\",len(test_instances))\n",
        "print(\"Unaltered Test Instances: {}\".format(len(test_instances_unaltered)))\n",
        "print(\"Test Label:\", len(test_labels))\n",
        "print(\"Train Instances:\",len(train_instances))\n",
        "print(\"Unaltered Train Instances: {}\".format(len(train_instances_unaltered)))\n",
        "print(\"Train Label:\", len(train_labels))\n",
        "\n",
        "print(\"\\nExpected Combined Length:\", len(pcl_indeces)+len(nonpcl_indeces_shortened))\n",
        "print(\"Actual Combined Length:\", len(train_instances)+ len(test_instances))\n",
        "\n",
        "print(\"\\nTransform labels to 1 and 0\\n1=pcl, 0 = no pcl \")\n",
        "#Replace 1->0, 3and 4 to 1. Only other possible input is 0 which should remain\n",
        "for i in range(len(train_labels)):\n",
        "    if train_labels[i] == 1:\n",
        "        train_labels[i] = 0\n",
        "    elif train_labels[i] == 3:\n",
        "        train_labels[i] = 1\n",
        "    elif train_labels[i] == 4:\n",
        "        train_labels[i] = 1\n",
        "for i in range(len(test_labels)):\n",
        "    if test_labels[i] == 1:\n",
        "        test_labels[i] = 0\n",
        "    elif test_labels[i] == 3:\n",
        "        test_labels[i] = 1\n",
        "    elif test_labels[i] == 4:\n",
        "        test_labels[i] = 1\n",
        "\n",
        "print(\"\\n\\nFinal Diagnostic Information After seperation:\")\n",
        "print(\"Are train label and instances the same length? {}\\t Length is {}\".format(len(train_instances)==len(train_labels), len(train_instances)))\n",
        "print(\"Are test label and instances the same length? {}\\t Length is {}\".format(len(test_instances)==len(test_labels), len(test_instances)))\n",
        "print(\"Proportion of PCL in train: {}\".format(train_labels.count(1)/len(train_labels)))\n",
        "print(\"Proportion of PCL in test: {}\".format(test_labels.count(1)/len(test_labels)))\n",
        "\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80.0% cutoff at index: 681\n",
            "\n",
            "Test Instances: 342\n",
            "Unaltered Test Instances: 342\n",
            "Test Label: 342\n",
            "Train Instances: 1362\n",
            "Unaltered Train Instances: 1362\n",
            "Train Label: 1362\n",
            "\n",
            "Expected Combined Length: 1704\n",
            "Actual Combined Length: 1704\n",
            "\n",
            "Transform labels to 1 and 0\n",
            "1=pcl, 0 = no pcl \n",
            "\n",
            "\n",
            "Final Diagnostic Information After seperation:\n",
            "Are train label and instances the same length? True\t Length is 1362\n",
            "Are test label and instances the same length? True\t Length is 342\n",
            "Proportion of PCL in train: 0.5\n",
            "Proportion of PCL in test: 0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY0rJXPdKcSa"
      },
      "source": [
        "TF - IDF Implementation          Adapted from: https://sanjayasubedi.com.np/machinelearning/nlp/text-classification-with-sklearn/ and https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.YXgb1L_MJH4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "za_d2wb1Kkxf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1b59925-4e9e-48dd-a159-7602170dde8d"
      },
      "source": [
        "# # Check if when training you are supposed to only include postive(PCL containing) textx\n",
        "\n",
        "if TF_IDF:\n",
        "  #Creates tfidf scores for the train values, will be called later\n",
        "\n",
        "  #Initialize Vectorizer for Tfidf which will choose 1000 most informative features\n",
        "  vectorizer_pcl = TfidfVectorizer(max_features = tfidf_maxfeatures, use_idf=True)\n",
        "  vectorizer_nopcl = TfidfVectorizer(max_features = tfidf_maxfeatures, use_idf=True)\n",
        "\n",
        "  tfidf_pcl_train = []\n",
        "  tfidf_nopcl_train = []\n",
        "  for i in range(len(train_labels)):\n",
        "    if train_labels[i] == 1:\n",
        "      tfidf_pcl_train.append(train_instances[i])\n",
        "    else:\n",
        "      tfidf_nopcl_train.append(train_instances[i])\n",
        "  train_vectorized_pcl = vectorizer_pcl.fit_transform(tfidf_pcl_train)\n",
        "  train_vectorized_nopcl = vectorizer_nopcl.fit_transform(tfidf_nopcl_train)\n",
        "\n",
        "  #This is example of creating normalarray which will need to be appended to BERT array before NN\n",
        "  #print(vectorizer.transform([test_instances[0]]).toarray())\n",
        "  print(\"TFIDF Matrix Created\")\n",
        "else:\n",
        "  print(\"TFIDF Not created due to earlier parameters\")\n",
        "\n",
        "\n",
        "if TF_IDF:   #Behind if statement because only want to run if TF-IDF being created\n",
        "  # Create Array of tfidf result for each train instances:\n",
        "  train_tfidf_pcl = vectorizer_pcl.transform(train_instances).toarray()\n",
        "  train_tfidf_nopcl = vectorizer_nopcl.transform(train_instances).toarray()\n",
        "\n",
        "  #COMBINE THOSE 2 together to be train_tfidf\n",
        "  train_tfidf = []\n",
        "  for i in range(len(train_tfidf_pcl)):\n",
        "    to_append = np.concatenate((train_tfidf_pcl[i], train_tfidf_nopcl[i]), axis=None)\n",
        "    train_tfidf.append(to_append)\n",
        "  print(\"TFIDF Results For Train Data Complete\")\n",
        "  print(\"Dimension is {} x {}\".format(len(train_tfidf), len(train_tfidf[0])))\n",
        "\n",
        "  # Make Test array right now too, cause why not\n",
        "  test_tfidf_pcl = vectorizer_pcl.transform(test_instances).toarray()\n",
        "  test_tfidf_nopcl = vectorizer_nopcl.transform(test_instances).toarray()\n",
        "\n",
        "  #COMBINE THOSE 2 together to be test_tfidf\n",
        "  test_tfidf = []\n",
        "  for i in range(len(test_tfidf_pcl)):\n",
        "    to_append = np.concatenate((test_tfidf_pcl[i], test_tfidf_nopcl[i]), axis=None)\n",
        "    test_tfidf.append(to_append)\n",
        "\n",
        "  print(\"TFIDF Results For Test Data Complete\")\n",
        "  print(\"Dimension is {} x {}\".format(len(test_tfidf), len(test_tfidf[0])))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFIDF Not created due to earlier parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUkrwe64b7E6"
      },
      "source": [
        "BERT For Classification Implementation: Adapted from https://mccormickml.com/2019/07/22/BERT-fine-tuning/#12-installing-the-hugging-face-library, also took notes from Amy Olex's Bert Implementation\n",
        "\n",
        "This Section Finds and sets the true max length used in the classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qsBvRmJcF0E",
        "outputId": "cf6325c5-9da3-402c-c251-3503ddc098e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if BERT:\n",
        "  if pre_process_BERT == False:\n",
        "    #Set train instances and test instances to the unmodified versions (no preproccessing has occured)\n",
        "    print(\"Bert embeddings formed from unaltered sentences\")\n",
        "    train_instances = train_instances_unaltered\n",
        "    test_instances = test_instances_unaltered\n",
        "  else: \n",
        "    print(\"Bert embeddings formed from pre-processed sentences\")\n",
        "\n",
        "  # Load the BERT tokenizer. Will be used to Re-tokenize data\n",
        "  print('Loading BERT tokenizer...')\n",
        "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "\n",
        "  # ''' Shows how Bert Tokenized\n",
        "  # # Print the original sentence.\n",
        "  # print(' Original: ', train_instances[0])\n",
        "  # # Print the sentence split into tokens.\n",
        "  # print('Tokenized: ', tokenizer.tokenize(train_instances[0]))\n",
        "  # # Print the sentence mapped to token ids.\n",
        "  # print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(train_instances[0])))\n",
        "  # '''\n",
        "\n",
        "  max_obs_len = 0\n",
        "  # Check all train data and find highest token character\n",
        "  for text in train_instances:\n",
        "      # Tokenize the text and add `[CLS]` and `[SEP]` tokens.\n",
        "      input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "      # Update the maximum sentence length.\n",
        "      max_obs_len = max(max_obs_len, len(input_ids))\n",
        "\n",
        "  for text in test_instances:\n",
        "    # Check all train data and find highest token character\n",
        "      input_ids = tokenizer.encode(text, add_special_tokens=True)\n",
        "      # Update the maximum sentence length.\n",
        "      max_obs_len = max(max_obs_len, len(input_ids))\n",
        "\n",
        "\n",
        "  print('Max Observed sentence length: ', max_obs_len)\n",
        "\n",
        "  #Select the minimum of the maximum lengths. BERT will truncate any sentances longer than this. \n",
        "  max_len = min(max_obs_len, max_len)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bert embeddings formed from pre-processed sentences\n",
            "Loading BERT tokenizer...\n",
            "Max Observed sentence length:  118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McNS3P81svfa"
      },
      "source": [
        "Training Tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruRBr2W1sxLX",
        "outputId": "0ddf0d98-a007-4356-dfc3-8820fcdfdc24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if BERT:\n",
        "  print(\"Tokenize all of the training sentences and map the tokens to thier word IDs.\")\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  train_input_ids = []\n",
        "  train_attention_masks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for text in train_instances:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length'\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "      encoded_dict = tokenizer.encode_plus(\n",
        "                          text,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = max_len,           # Pad & truncate all sentence\n",
        "                          pad_to_max_length=True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.    \n",
        "      train_input_ids.append(encoded_dict['input_ids'])\n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      train_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "\n",
        "  train_input_ids = torch.stack(train_input_ids).squeeze()\n",
        "  train_attention_masks = torch.stack(train_attention_masks).squeeze()\n",
        "  train_label_tensor = torch.tensor(train_labels)\n",
        "  \n",
        "  #Make train dataset\n",
        "  #train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "\n",
        "  # Check dimensionality (should be same)\n",
        "  # print(train_input_ids.shape)\n",
        "  # print(train_attention_masks.shape)\n",
        "  # print(train_label_tensor.shape)\n",
        "\n",
        "  train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_label_tensor)\n",
        "\n",
        "  # ''' Print to see what tensor looks like\n",
        "  # # Print sentence 0, now as a list of IDs.\n",
        "  # print('Original: ', train_instances[0])\n",
        "  # print('Token IDs:', train_input_ids[0])\n",
        "  # '''"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize all of the training sentences and map the tokens to thier word IDs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cac8Ed9Gs-y2"
      },
      "source": [
        "Test Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M8R2oxvatAdq",
        "outputId": "136891bc-3578-4d98-a2f0-4fea23b380de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if BERT:\n",
        "  print(\"Tokenize all of the testing sentences and map the tokens to thier word IDs.\")\n",
        "  # Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "  test_input_ids = []\n",
        "  test_attention_masks = []\n",
        "\n",
        "  # For every sentence...\n",
        "  for text in test_instances:\n",
        "      # `encode_plus` will:\n",
        "      #   (1) Tokenize the sentence.\n",
        "      #   (2) Prepend the `[CLS]` token to the start.\n",
        "      #   (3) Append the `[SEP]` token to the end.\n",
        "      #   (4) Map tokens to their IDs.\n",
        "      #   (5) Pad or truncate the sentence to `max_length'\n",
        "      #   (6) Create attention masks for [PAD] tokens.\n",
        "      test_encoded_dict = tokenizer.encode_plus(\n",
        "                          text,                      # Sentence to encode.\n",
        "                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                          max_length = max_len,           # Pad & truncate all sentences.\n",
        "                          truncation = True,\n",
        "                          pad_to_max_length=True,\n",
        "                          return_attention_mask = True,   # Construct attn. masks.\n",
        "                          return_tensors = 'pt',     # Return pytorch tensors.\n",
        "\n",
        "                    )\n",
        "      \n",
        "      # Add the encoded sentence to the list.    \n",
        "      test_input_ids.append(test_encoded_dict['input_ids'])\n",
        "      \n",
        "      # And its attention mask (simply differentiates padding from non-padding).\n",
        "      test_attention_masks.append(test_encoded_dict['attention_mask'])\n",
        "\n",
        "  # Convert the lists into tensors.\n",
        "\n",
        "  test_input_ids = torch.stack(test_input_ids).squeeze()\n",
        "  test_attention_masks = torch.stack(test_attention_masks).squeeze()\n",
        "  test_labels_tensor = torch.tensor(test_labels)\n",
        "\n",
        "  #Make test dataset\n",
        "  test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels_tensor)\n",
        "\n",
        "  # '''Test the Output\n",
        "  # # Print sentence 0, now as a list of IDs.\n",
        "  # print('Original: ', test_instances[0])\n",
        "  # print('Token IDs:', test_input_ids[0])\n",
        "  # '''"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenize all of the testing sentences and map the tokens to thier word IDs.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2218: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kab6IqzpueFu"
      },
      "source": [
        "Create Data Loader for train and test datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZErKJI6DueXQ",
        "outputId": "f64c7703-c526-4818-9125-7905cbff4e2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if BERT:\n",
        "  #Create the DataLoaders for our training and validation sets.\n",
        "  #We'll take training samples in random order. \n",
        "  print(\"Making Train dataloader\")\n",
        "  train_dataloader = DataLoader(\n",
        "              train_dataset,  # The training samples.\n",
        "              sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "              batch_size = batch_size_param # Trains with this batch size.\n",
        "          )\n",
        "\n",
        "  print(\"Making Test dataloader\")\n",
        "  # For validation the order doesn't matter, so we'll just read them sequentially.\n",
        "  validation_dataloader = DataLoader(\n",
        "              test_dataset, # The validation samples.\n",
        "              sampler = SequentialSampler(test_dataset), # Pull out batches sequentially.\n",
        "              batch_size = batch_size_param # Evaluate with this batch size.\n",
        "          )"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Making Train dataloader\n",
            "Making Test dataloader\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wgBfbAwtzl4K"
      },
      "source": [
        "Loading Bert and setting parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnzgGeZ0zoqk",
        "outputId": "2cc5ea36-e748-4135-dd09-fbe9f709237b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if BERT:\n",
        "  # Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "  # linear classification layer on top. \n",
        "\n",
        "  print(\"Loading BERT\")\n",
        "  model = BertModel.from_pretrained(\n",
        "      \"bert-base-uncased\" # Use the 12-layer BERT model, with an uncased vocab.\n",
        "  )\n",
        "\n",
        "  # Tell pytorch to run this model on the GPU.\n",
        "  model.to(device)\n",
        "\n",
        "#Set seeds if random seed it set\n",
        "if random_seed:\n",
        "  np.random.seed(random_seed)\n",
        "  torch.manual_seed(random_seed)\n",
        "  torch.cuda.manual_seed_all(random_seed)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BERT\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KwU3Ihs3SQM"
      },
      "source": [
        "Get BERT embeddings for train data\n",
        "BERT embedding is average vector for sentence. Result has length 768."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZgtURrH3T8j",
        "outputId": "54eeb6c4-72d0-4df1-d991-1e6ca3d68fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "if BERT:\n",
        "  model.eval()\n",
        "\n",
        "  #print(b_input_ids[0])\n",
        "\n",
        "  train_word_embed_eachinstance = []\n",
        "  count = 0\n",
        "  print(\"There will be {} batches\".format(len(train_dataloader)))\n",
        "  for batch in train_dataloader:\n",
        "    if count%5==0 and count!=0:\n",
        "      print(\"On batch number {}\".format(count))\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    \n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model_bert not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "        \n",
        "        #train_cls_head.append(outputs[1])\n",
        "        #train_word_embed_eachinstance.append(outputs[0])\n",
        "        #print(len(outputs[0]))\n",
        "        for sentence in outputs[0]:\n",
        "          #print(len(sentence))\n",
        "          sentence_embed = []\n",
        "          for embed_index in range(len(sentence[0])):\n",
        "            index_total = 0\n",
        "            for word in sentence:\n",
        "              index_total += word[embed_index]\n",
        "            sentence_embed.append(index_total / len(word))\n",
        "          train_word_embed_eachinstance.append(sentence_embed)\n",
        "        count += 1\n",
        "\n",
        "  #Variable still called train_cls_head, but is average of every words embedding across sentence\n",
        "  train_cls_head = torch.FloatTensor(train_word_embed_eachinstance)\n",
        "  print(\"Number of Bert Embeddings created: {}\\t\\t Should equal number of training instances\".format(len(train_cls_head)))\n",
        "  print(\"Length of a single BERT embedding: {}\\t\\t Should be 768\".format(len(train_cls_head[0])))\n",
        "\n",
        "  print(\"Bert Vectors Created for all Train instances\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There will be 86 batches\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-79a1f1a4f5d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Forward pass, calculate logit predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb_input_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m#train_cls_head.append(outputs[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m         )\n\u001b[1;32m   1008\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    588\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m                 )\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m         layer_output = apply_chunking_to_forward(\n\u001b[0;32m--> 512\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    513\u001b[0m         )\n\u001b[1;32m    514\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlayer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2360\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qS3NAr1DSV8"
      },
      "source": [
        "Get BERT embeddings for the test instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7rXd4PpDVMU"
      },
      "source": [
        "if BERT:\n",
        "  test_word_embed_eachinstance = []\n",
        "  count = 0\n",
        "  print(\"There will be {} batches\".format(len(validation_dataloader)))\n",
        "  for batch in validation_dataloader:\n",
        "    if count%5==0 and count!=0:\n",
        "      print(\"On batch number {}\".format(count))\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "    t_input_ids, t_input_mask, b_labels = batch\n",
        "    \n",
        "    # Telling the model_bert not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "        # Forward pass\n",
        "        outputs = model(t_input_ids, token_type_ids=None, attention_mask=t_input_mask)\n",
        "        \n",
        "        for sentence in outputs[0]:\n",
        "          #print(len(sentence))\n",
        "          sentence_embed = []\n",
        "          for embed_index in range(len(word)):\n",
        "            index_total = 0\n",
        "            for word in sentence:\n",
        "              index_total += word[embed_index]\n",
        "            sentence_embed.append(index_total / len(word))\n",
        "          test_word_embed_eachinstance.append(sentence_embed)\n",
        "        \n",
        "        count += 1\n",
        "\n",
        "\n",
        "  test_cls_head = torch.FloatTensor(test_word_embed_eachinstance)\n",
        "  print(\"Number of Bert Embeddings created: {}\\t\\t Should equal number of test instances\".format(len(test_cls_head)))\n",
        "  print(\"Length of a single BERT embedding: {}\\t\\t Should be 768\".format(len(test_cls_head[0])))\n",
        "  print(\"Bert Vectors Embeddings Created for all Test instances\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u8a2yUFp0Rk"
      },
      "source": [
        "In This block, Will combine the vectors produced by tfidf with the tensors/vectors created by BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXqFrFgvp6D5"
      },
      "source": [
        "train_combined_vectors = []\n",
        "test_combined_vectors = []\n",
        "\n",
        "#IF doing both BERT and Tf-IDF\n",
        "if TF_IDF and BERT: \n",
        "  #Turn BERT tensor to numpy array for easier work\n",
        "  #train_cls_head.numpy() \n",
        "  print(\"TFIDF and BERT Results Same Length: {}\".format(len(train_tfidf) == len(train_cls_head)))\n",
        "  #Check that these are same length, if they arent, something went wrong\n",
        "\n",
        "  if len(train_tfidf) != len(train_cls_head):\n",
        "    print(\"Train Vectors are not the same Length, errors have occured.\\n Quitting Process\")\n",
        "  if len(test_tfidf) != len(test_cls_head):\n",
        "    \n",
        "    print(\"Train Vectors are not the same Length, errors have occured.\\n Quitting Process\")\n",
        "\n",
        "  #Combine Train TFIDF Vector into 1 vector\n",
        "  for i in range(len(train_tfidf)):\n",
        "    tfidf_cur_index = train_tfidf[i]\n",
        "    bert_cur_index = train_cls_head[i]\n",
        "    # print(len(tfidf_cur_index))\n",
        "    # print(len(bert_cur_index))\n",
        "    combined_entry = np.concatenate((tfidf_cur_index, bert_cur_index))\n",
        "    train_combined_vectors.append(combined_entry)\n",
        "  print(\"Train TFIDF and Combined Vectors Created\")\n",
        "\n",
        "#Combine Train TFIDF Vector into 1 vector\n",
        "  for i in range(len(test_tfidf)):\n",
        "    tfidf_cur_index = test_tfidf[i]\n",
        "    bert_cur_index = test_cls_head[i]\n",
        "    # print(len(tfidf_cur_index))\n",
        "    # print(len(bert_cur_index))\n",
        "    combined_entry = np.concatenate((tfidf_cur_index, bert_cur_index))\n",
        "    test_combined_vectors.append(combined_entry)\n",
        "  print(\"Test TFIDF and Combined Vectors Created\")\n",
        "\n",
        "\n",
        "# If BERt is not selected:\n",
        "elif TF_IDF and not BERT:\n",
        "  train_cls_head = [[]]\n",
        "  test_cls_head = [[]]\n",
        "  for i in range(len(train_tfidf)):\n",
        "    tfidf_cur_index = train_tfidf[i]\n",
        "    train_combined_vectors.append(tfidf_cur_index)\n",
        "  for i in range(len(test_tfidf)):\n",
        "    tfidf_cur_index = test_tfidf[i]\n",
        "    test_combined_vectors.append(tfidf_cur_index)\n",
        "\n",
        "#IF ONLY Bert is selected\n",
        "elif BERT and not TF_IDF:\n",
        "  train_tfidf = [[]]\n",
        "  test_tfidf = [[]]\n",
        "  for i in range(len(train_cls_head)):\n",
        "    bert_cur_index = train_cls_head[i].numpy()\n",
        "    train_combined_vectors.append(bert_cur_index)\n",
        "  for i in range(len(test_cls_head)):\n",
        "    bert_cur_index = test_cls_head[i].numpy()\n",
        "    test_combined_vectors.append(bert_cur_index)\n",
        "\n",
        "print(\"Expected Length is Bert vector {} + TFIDF vector {} for a total length of {}\".format(len(train_cls_head[0]), len(train_tfidf[0]), (len(train_tfidf[0])+len(train_cls_head[0]))))\n",
        "print(\"Combined Train Vector is {} Long\".format(len(train_combined_vectors[0])))\n",
        "print(\"Combined Test Vector is {} Long\".format(len(test_combined_vectors[0])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWaX-DMLxqH3"
      },
      "source": [
        "Neural Network Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpGDCRrqxue0"
      },
      "source": [
        "#Create training loader for neural netowrk\n",
        "train_combined_vect_nn = torch.tensor(train_combined_vectors).squeeze()\n",
        "train_label_tensor_nn = torch.tensor(train_labels)\n",
        "\n",
        "\n",
        "train_data_nn = TensorDataset(train_combined_vect_nn, train_label_tensor_nn)\n",
        "train_sampler_nn = RandomSampler(train_data_nn)\n",
        "train_dataloader_nn = DataLoader(train_data_nn, sampler=train_sampler_nn, batch_size = batch_size_nn)\n",
        "print(\"Train Loader Created\")\n",
        "\n",
        "#BUILD NEURAL NETWORK WITH SOFTMAX CLASSICATION OUTPUT\n",
        "#NUMBER OF INPUTS = length of combined vectors\n",
        "num_inputs = len(train_combined_vectors[0]) \n",
        "\n",
        "# Build a feed-forward network\n",
        "# one_third_cut = int(num_inputs/3)\n",
        "# nn_model = nn.Sequential(nn.Linear(num_inputs, num_inputs),\n",
        "#                          nn.ReLU(),\n",
        "#                          nn.Linear(num_inputs, one_third_cut *2),\n",
        "#                          nn.ReLU(),\n",
        "#                          nn.Linear(one_third_cut *2, int(2* one_third_cut / 3 )),\n",
        "#                          nn.ReLU(),\n",
        "#                          nn.Linear(int(2* one_third_cut / 3 ), 1),\n",
        "#                          nn.Sigmoid())\n",
        "\n",
        "# nn_model = nn.Sequential(nn.Linear(num_inputs, int(num_inputs/2)),\n",
        "#                          nn.ReLU(),\n",
        "#                          nn.Linear(int(num_inputs/2), 1),\n",
        "#                          nn.Sigmoid())\n",
        "\n",
        "\n",
        "nn_model = nn.Sequential(nn.Linear(num_inputs, 1),\n",
        "                           nn.Sigmoid())\n",
        " \n",
        "print(nn_model)\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = optim.Adam(nn_model.parameters(), lr = learning_rate)\n",
        "\n",
        "# Train the Neural Network\n",
        "nn_model.train()\n",
        "\n",
        "for e in range(num_epochs):\n",
        "    train_pred, train_act = [], []\n",
        "    print(\"Epoch {}\".format(e))\n",
        "    running_loss = 0\n",
        "    nn_model.train()\n",
        "    for step, batch in enumerate(train_dataloader_nn):\n",
        "        # if step%10==0:\n",
        "        #  print(\"Step: \" + str(step))\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        vectors, labels = batch\n",
        "        # Training pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = nn_model(vectors.float()\n",
        "        )\n",
        "        for index in range(len(output)):\n",
        "          if output[index] > 0.5:\n",
        "            train_pred.append(1)\n",
        "            train_act.append(labels[index])\n",
        "          else:\n",
        "            train_pred.append(0)\n",
        "            train_act.append(labels[index])\n",
        "\n",
        "        labels = labels.unsqueeze(1).float()\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        running_loss += loss.item()\n",
        "        \n",
        "    else:\n",
        "        correct = 0\n",
        "        for i in range(len(train_pred)):\n",
        "          if train_pred[i] == train_act[i]:\n",
        "            correct +=1\n",
        "        print(f\"Average Training loss Over The Epoch: {running_loss/len(train_dataloader_nn)}\")\n",
        "        print(\"Epoch Accuracy was {}\".format(correct / len(train_pred)))\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cv4oydrDHHR"
      },
      "source": [
        "Test the Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LG4JvKW2DHW1"
      },
      "source": [
        "#Create Test loader for neural netowrk\n",
        "test_combined_vect_nn = torch.tensor(test_combined_vectors).squeeze()\n",
        "test_label_tensor_nn = torch.tensor(test_labels)\n",
        "\n",
        "# print(test_combined_vect_nn.size())\n",
        "# print(test_label_tensor_nn.size())\n",
        "\n",
        "test_data_nn = TensorDataset(test_combined_vect_nn, test_label_tensor_nn)\n",
        "test_sampler_nn = RandomSampler(test_data_nn)\n",
        "test_dataloader_nn = DataLoader(test_data_nn, sampler=test_sampler_nn, batch_size = 1)\n",
        "print(\"length dataloader {}\".format(len(test_dataloader_nn)))\n",
        "# Tracking variables for Neural Network Test (MODIFY TO CHANGE WHAT IS REPORTED)\n",
        "\n",
        "\n",
        "predictions, true_labels = [], []\n",
        "\n",
        "count=0\n",
        "# Evaluate data for one epoch\n",
        "with torch.no_grad():\n",
        "  nn_model.eval()\n",
        "  for batch in test_dataloader_nn:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    t_vectors, t_labels = batch\n",
        "\n",
        "    output = nn_model(t_vectors.float())\n",
        "    \n",
        "    if output > 0.5:\n",
        "      predictions.append(1)\n",
        "      true_labels.append(t_labels.item())\n",
        "      #print(1)\n",
        "    else:\n",
        "      predictions.append(0)\n",
        "      true_labels.append(t_labels.item())\n",
        "      #print(0)\n",
        "\n",
        "    \n",
        "    count+=1\n",
        "\n",
        "print(\"Testing complete!\")\n",
        "true_pos, true_neg, false_pos, false_neg = 0, 0, 0, 0\n",
        "\n",
        "for i in range(len(predictions)):\n",
        "  if predictions[i] == true_labels[i] and predictions[i] == 1:\n",
        "    true_pos +=1\n",
        "  if predictions[i] == true_labels[i] and predictions[i] == 0:\n",
        "    true_neg +=1\n",
        "  if predictions[i] != true_labels[i] and predictions[i] == 1:\n",
        "    false_pos += 1\n",
        "  if predictions[i] != true_labels[i] and predictions[i] == 0:\n",
        "    false_neg +=1\n",
        "\n",
        "print(\"Accuracy Equals \\t{}\".format((true_pos+true_neg)/(true_pos+true_neg+false_pos+false_neg)))\n",
        "precision = true_pos/(true_pos+false_pos)\n",
        "print(\"Precision Equals \\t{}\".format(precision))\n",
        "recall = true_pos/(true_pos+false_neg)\n",
        "print(\"Recall Equals\\t{}\".format(recall))\n",
        "print(\"F-Score Equals \\t{}\".format(2*( (precision * recall) / (precision + recall) ) ) )\n",
        "\n",
        "print((true_pos+true_neg+false_pos+false_neg))\n",
        "print(len(predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
